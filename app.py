'''
This code is generated by Gemini. The main goal of this project is to make a simple
RAG workflow using LangChain and Streamlit, allowing users to upload a PDF document,
The intent of this code is for demonstration purposes only,
showcasing how to build a Retrieval-Augmented Generation (RAG) application.
'''

import streamlit as st
import os
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate

# --- SETUP ---
# Load environment variables from .env file for security
load_dotenv()


# --- CORE RAG LOGIC ---

def get_rag_chain(pdf_doc):
    """
    Creates a Retrieval-Augmented Generation (RAG) chain from a PDF document.

    Args:
        pdf_doc (list): A list of uploaded PDF file objects.

    Returns:
        A LangChain retrieval chain object ready to be invoked.
    """
    # 1. Load Documents
    # PyPDFLoader handles reading the text content from the PDF.
    docs = []
    for pdf in pdf_doc:
        loader = PyPDFLoader(pdf)
        docs.extend(loader.load())

    # 2. Split Documents into Chunks
    # This splitter breaks the document into smaller, manageable chunks.
    # This is crucial for the vector store's effectiveness.
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(docs)

    # 3. Create Embeddings and Vector Store
    # OpenAIEmbeddings converts text chunks into numerical vectors.
    # FAISS creates an in-memory vector store for fast similarity searches.
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(split_docs, embeddings)

    # 4. Create the Retriever
    # The retriever fetches relevant document chunks from the vector store.
    retriever = vectorstore.as_retriever()

    # 5. Define the Prompt Template
    # This template structures the input for the LLM, telling it to use
    # the retrieved context to answer the user's question.
    prompt_template = """
    Answer the user's question based only on the following context.
    If you can't find the answer in the context, just say that you don't know.

    Context:
    {context}

    Question:
    {input}
    """
    prompt = ChatPromptTemplate.from_template(prompt_template)

    # 6. Initialize the LLM
    llm = ChatOpenAI(model="gpt-4.1")

    # 7. Create the RAG Chain
    # This chain combines all the elements: it takes a question, retrieves
    # relevant documents, formats the prompt, and sends it to the LLM.
    document_chain = create_stuff_documents_chain(llm, prompt)
    retrieval_chain = create_retrieval_chain(retriever, document_chain)

    return retrieval_chain


# --- STREAMLIT UI ---

# Set page configuration
st.set_page_config(page_title="Chat with PDF üìÑ", layout="wide")

# Set the title of the app
st.title("Chat with Your PDF üìÑ")
st.write("Upload a PDF file and ask questions about its content.")

# Sidebar for API key and file upload
with st.sidebar:
    st.header("Setup")
    # Get OpenAI API Key from user
    api_key = st.text_input("Enter your OpenAI API Key:", type="password")
    if api_key:
        os.environ["OPENAI_API_KEY"] = api_key
        st.success("API Key set successfully! üöÄ")

    # PDF file uploader
    uploaded_file = st.file_uploader("Upload your PDF", type="pdf")

    # Process button
    process_button = st.button("Process Document")

# Initialize session state for chat history and RAG chain
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Hello! Please upload a PDF and process it to start chatting."}]

if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None

# Handle document processing
if process_button:
    if not api_key:
        st.error("Please enter your OpenAI API Key in the sidebar.")
    elif not uploaded_file:
        st.error("Please upload a PDF file.")
    else:
        with st.spinner("Processing your document... This may take a moment. ‚è≥"):
            # To handle the file object correctly, we need to save it temporarily
            with open(uploaded_file.name, "wb") as f:
                f.write(uploaded_file.getbuffer())

            # Create the RAG chain
            st.session_state.rag_chain = get_rag_chain([uploaded_file.name])

            # Clean up the temporary file
            os.remove(uploaded_file.name)

        st.success("Document processed! You can now ask questions.")
        # Clear previous chat history and add a new welcome message
        st.session_state.messages = [
            {"role": "assistant", "content": f"I'm ready! Ask me anything about '{uploaded_file.name}'."}]

# Display chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Handle user input
if user_question := st.chat_input("Ask a question about the PDF:"):
    if st.session_state.rag_chain is None:
        st.warning("Please upload and process a document first.")
    else:
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.markdown(user_question)

        # Get the assistant's response
        with st.spinner("Thinking... ü§î"):
            response = st.session_state.rag_chain.invoke({"input": user_question})
            answer = response["answer"]

            # Add assistant response to chat history
            st.session_state.messages.append({"role": "assistant", "content": answer})
            with st.chat_message("assistant"):
                st.markdown(answer)